# -*- org-latex-minted-options: (("breaklines" "true") ("breakanywhere" "true") ("fontsize" "\\footnotesize")); -*-
#+title: Cardano 92 Perf regression follow-up
#+latex_class_options: [10pt]
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage{xcolor}
#+latex_header_extra: \definecolor{LightGray}{gray}{.96}
#+latex_header_extra: \setminted{bgcolor=LightGray}
#+PROPERTY: header-args:R :session *cardano-perf-report* :cache yes :dir ./

* Executive Summary

- From inspecting the 92 regression the DevX team made several recommendations
  for code changes to ~cardano-ledger-core~

- This document tests these changes against with ~beacon~ and ~db-analyzer~.

- Our findings are:

  - ~beacon~ and ~db-analyzer~ do observe changes in ~cardano-core-ledger~.

  - This method does _independently_ observe the 92 regression.

  - Removing ~FailT~ has a negative impact on performance

  - Splitting ~UMElem~ to take advantage of pointer tagging improves performance by


* Background

** General Goal

- The larger goal is to be able to predict the performance of the ledger
  operations before shipping the code.

- ~db-analyzer~  and ~beacon~ are attempts to do this by replaying the
  state changes that took place. They replay the ledger state by using ledger
  operations to make queries on the ledger state.

- The ledger-ops are essentially queries

** ~db-analyzer~ and ~beacon~

- db-analyzer is the tool that is doing the work

- beacon is just a convienient wrapper for data generation and plotting.

* Methodology

- This data is generated by [[https://github.com/input-output-hk/ouroboros-consensus/tree/main/ouroboros-consensus-cardano#saving-a-snapshot][db-analyzer]] and [[https://github.com/input-output-hk/ouroboros-consensus-tools][beacon]]. Using a handcrafted chainDB.
  The handcrafted chainDB has a density of 3 (epoch length of 600 / fill rate of
  1 out of every 20th slot), in contrast ~mainnet~ has a fill rate of one slot
  200th slot. Therefore the handcrafted /chainDB is a stress test/ of the ledger
  ops.

- The handcrafted chainDB has two blocks, which yields 

    #+name: num-observations
    #+begin_src R
    nrow(df810_baseline)
    #+end_src

    #+RESULTS[0b5c315f8449b719e466662d9e074aa6b4aee56b]: num-observations
    : 122

    call_num-observations() observations per run. 

  - Furthermore, each observation belongs to a specific slot id, which means we
    have statistically /paired/ data. Paired data occurs when an observation is
    not independent between runs of an experiment. For example, if one was
    testing a diet pill on 10 mice, mouse 1 is uniquely different than mouse 4,
    and so the dataset will be 10 observations before the diet pill, and 10
    after. Each observation belongs to a unique mouse and the experiment is
    interested in how the pill changed each mouse individually, e.g., how did
    mouse 1 change. Thus comparing mouse 1 to mouse 4 is not a meaningful
    comparison. We are dealing with a similar kind of data. Each observation
    belongs to a unique and not-independently sampled slot id. For example, slot
    id 63 might always be more performance intensive than slot 73, and so it
    would be incorrect to use statistical tests that assume a random sample.
    Thus we must use statistical tests that account for paired data.

  - Next this data is not normally distributed:

    #+name: normality-test
    #+begin_src R :exports both :results output
    shapiro.test(df810_baseline$totalTime)
    #+end_src

    #+RESULTS[c42759cb933e6bc6f606d1f2d7b31213628a564f]: normality-test
    : 
    : 	Shapiro-Wilk normality test
    : 
    : data:  df810_baseline$totalTime
    : W = 0.42007, p-value < 0.00000000000000022

    The baseline data for ~totalTime~ fails the normality test with a p-value of
    2.2e15. Even if we sample the data to exploit the central limit theorem we
    get a non-normal sample:

    #+name: normality-test-sample
    #+begin_src R :exports both :results output
    shapiro.test(sample_n(df810_baseline, 50)$totalTime)
    #+end_src

    #+RESULTS[ecd0c92affca7c988ce9a3c90a8c0444b2b66187]: normality-test-sample
    : 
    : 	Shapiro-Wilk normality test
    : 
    : data:  sample_n(df810_baseline, 50)$totalTime
    : W = 0.50456, p-value = 0.000000000009435

    Thus we use a [[https://www.statology.org/kruskal-wallis-test/][kruskall-wallace]] test to test if a change has a statistically
    significant effect and a [[http://sthda.com/english/wiki/paired-samples-wilcoxon-test-in-r][pairwise-wilcox-test]] to determine which change had
    which effect and in what direction (slower or faster).


* Introduction

    We'll be comparing three ghc versions: 810, 92, and 96; across three
    branches: the baseline, split UMElem, and removing the FailT library.

** The baseline

    The baseline branch is set to ouroboros-consensus commit
    ~e3917f684e8b60e7bfc453d6d8114b800bdf167d~, which is the release for
    ~node-8.5~. 

** Split UMElem

    The ledger uses a map data structure called ~UMap~ whose range is
    represented by a type called ~UMElem~ which looks like this: 
    #+begin_src haskell
    -- So,
    -- TEEEE means none of the components are present,
    -- TFEEE means only the reward-deposit pair is present,
    -- TEFEE means only the set of pointers is present,
    -- TEEFE means only the stake pool id is present. etc.
    -- TEEEF means only the voting delegatee id is present, and
    --
    -- The pattern 'UMElem' will correctly use the optimal constructor.
    data UMElem c
      = TEEEE
      | TEEEF !(DRep c)
      | TEEFE !(KeyHash 'StakePool c)
      | TEEFF !(KeyHash 'StakePool c) !(DRep c)
      | TEFEE !(Set Ptr)
      | TEFEF !(Set Ptr) !(DRep c)
      | TEFFE !(Set Ptr) !(KeyHash 'StakePool c)
      | TEFFF !(Set Ptr) !(KeyHash 'StakePool c) !(DRep c)
      | TFEEE {-# UNPACK #-} !RDPair
      | TFEEF {-# UNPACK #-} !RDPair !(DRep c)
      | TFEFE {-# UNPACK #-} !RDPair !(KeyHash 'StakePool c)
      | TFEFF {-# UNPACK #-} !RDPair !(KeyHash 'StakePool c) !(DRep c)
      | TFFEE {-# UNPACK #-} !RDPair !(Set Ptr)
      | TFFEF {-# UNPACK #-} !RDPair !(Set Ptr) !(DRep c)
      | TFFFE {-# UNPACK #-} !RDPair !(Set Ptr) !(KeyHash 'StakePool c)
      | TFFFF {-# UNPACK #-} !RDPair !(Set Ptr) !(KeyHash 'StakePool c) !(DRep c)
      deriving (Eq, Ord, Generic, NoThunks, NFData)
    #+end_src

    Notice that this data type has 16 constructors. The idea behind this branch
    is to split this data type into two types each with 8 constructors. With 8
    constructors GHC will utilize pointer tagging to scrutinize this data type.
    This should be much faster than the 16 constructor version, which will still
    perform the pointer tagging for the first 8 constructors, and then chase
    pointers to the info table of the heap object after that. 

    You can find the patch [[https://github.com/input-output-hk/cardano-ledger/compare/master...doyougnu:cardano-ledger:wip/perf-split-umelem][here]]. 

** Removing FailT

    The idea behind this patch is remove the polymorphism in
    ~Cardano.Ledger.Address~. This comes straight from the DevX analysis on the
    ghc-92 regression which found that a major difference on ghc-92 was a lack
    of specialization. ~FailT~ frequently showed up in that analysis and so
    removing it should pay off if the specialization was a contributing factor
    to the regression. This is especially the case because the code in
    ~Cardano.Ledger.Address~ uses a ~NOINLINE~ pragma for its ~fail~ function,
    which is known to
    [[https://gitlab.haskell.org/ghc/ghc/-/issues/22629][prevent
    specialization]].

    You can find the patch [[https://github.com/input-output-hk/cardano-ledger/compare/master...doyougnu:cardano-ledger:cardano-perf-regression/no-failT][here]].

** The analysis

   This analysis was done in R version:
   #+begin_src R :exports both :results output
   R.version.string
   #+end_src

   #+RESULTS[bb1eab72974fd4bf182d009f627e81e5c9f6c99c]:
   : [1] "R version 4.3.1 (2023-06-16)"

*** Loading and preparing the data

#+begin_src R :results silent
library("ggridges")
library("tidyverse")
library("rstatix")
library("tables")

options(scipen = 999)

data_dir <- "./data/"

load_data <- function(filename, ghc, branch) {
  read_tsv(paste(data_dir, filename, sep = "")) %>%
    mutate(GHC = as.factor(ghc), Branch = as.factor(branch))
}

## time units are nanoseconds
df810_baseline <- load_data("ledger-ops-cost-e3917f684e8b60e7bfc453d6d8114b800bdf167d-haskell810-from-63-nr-blocks-100000.csv", 810, "baseline")
df92_baseline  <- load_data("ledger-ops-cost-e3917f684e8b60e7bfc453d6d8114b800bdf167d-haskell-from-63-nr-blocks-100000.csv", 92, "baseline")
df96_baseline  <- load_data("ledger-ops-cost-e3917f684e8b60e7bfc453d6d8114b800bdf167d-haskell96-from-63-nr-blocks-100000.csv", 96, "baseline")

df810Split_umelem <- load_data("ledger-ops-cost-a929cd7616668b61bea38486b1641d5d45f13442-haskell810-from-63-nr-blocks-100000.csv", 810, "SplitUMElem")
df92Split_umelem  <- load_data("ledger-ops-cost-a929cd7616668b61bea38486b1641d5d45f13442-haskell-from-63-nr-blocks-100000.csv", 92, "SplitUMElem")
df96Split_umelem  <- load_data("ledger-ops-cost-a929cd7616668b61bea38486b1641d5d45f13442-haskell96-from-63-nr-blocks-100000.csv", 96, "SplitUMElem")

df810_noFailT <- load_data("ledger-ops-cost-6dc508fd5c0ddb73e4a5e01877dfcd698b1c1bd0-haskell810-from-63-nr-blocks-100000.csv", 810, "NoFailT")
df92_noFailT  <- load_data("ledger-ops-cost-6dc508fd5c0ddb73e4a5e01877dfcd698b1c1bd0-haskell-from-63-nr-blocks-100000.csv", 92, "NoFailT")
df96_noFailT  <- load_data("ledger-ops-cost-6dc508fd5c0ddb73e4a5e01877dfcd698b1c1bd0-haskell96-from-63-nr-blocks-100000.csv", 96, "NoFailT")

df <- bind_rows(
  df810_baseline, df92_baseline, df96_baseline,
  df810Split_umelem, df92Split_umelem, df96Split_umelem,
  df810_noFailT, df92_noFailT, df96_noFailT
) %>%
  mutate(TestCase = paste(GHC, Branch, sep = "_")) %>%
  arrange(slot)
#+end_src

#+RESULTS:

*** TODO Viewing the data

    Now we have our dataset, let's calculate some summary statistics of the
    samples:

    #+begin_src R :exports both :results output
    df %>%
    group_by(GHC,Branch) %>%
    select(totalTime) %>%
    get_summary_stats(type = "median_iqr")
    #+end_src

    #+RESULTS[72509926edbbb344737639a0e1049b53438ac466]:
    #+begin_example
    [1m[22mAdding missing grouping variables: `GHC`, `Branch`
    [90m# A tibble: 9 × 6[39m
      GHC   Branch      variable      n median    iqr
      [3m[90m<fct>[39m[23m [3m[90m<fct>[39m[23m       [3m[90m<fct>[39m[23m     [3m[90m<dbl>[39m[23m  [3m[90m<dbl>[39m[23m  [3m[90m<dbl>[39m[23m
    [90m1[39m 810   baseline    totalTime   122 [4m3[24m[4m2[24m200. [4m3[24m[4m7[24m113
    [90m2[39m 810   SplitUMElem totalTime   122 [4m3[24m[4m3[24m083  [4m3[24m[4m1[24m973.
    [90m3[39m 810   NoFailT     totalTime   122 [4m3[24m[4m2[24m521  [4m7[24m[4m1[24m903.
    [90m4[39m 92    baseline    totalTime   122 [4m6[24m[4m5[24m250. [4m3[24m[4m9[24m085.
    [90m5[39m 92    SplitUMElem totalTime   122 [4m6[24m[4m4[24m412. [4m3[24m[4m8[24m234.
    [90m6[39m 92    NoFailT     totalTime   122 [4m6[24m[4m8[24m834. [4m4[24m[4m1[24m404.
    [90m7[39m 96    baseline    totalTime   122 [4m3[24m[4m2[24m088. [4m2[24m[4m8[24m964.
    [90m8[39m 96    SplitUMElem totalTime   122 [4m3[24m[4m0[24m942. [4m2[24m[4m7[24m022.
    [90m9[39m 96    NoFailT     totalTime   122 [4m3[24m[4m2[24m738  [4m2[24m[4m8[24m118.
    #+end_example

    let's plot the distribution of ~totalTime~ for each
    ghc and branch. I'll use a [[https://en.wikipedia.org/wiki/Ridgeline_plot][ridgeline plot]] to observe changes in the
    distributions. Note that the x-axis is ~log10~ because we have an
    exponential distribution:

    #+begin_src R :exports both :results output graphics file :file plots/ridgeline.pdf
    p <- ggplot(df, aes(totalTime,
                        y = TestCase,
                        fill = GHC)) +
      geom_density_ridges(alpha = .6) +
      scale_x_log10() +
      xlab("TotalTime [ns]") +
      ylab("GHC_Branch") +
      theme_minimal()
    p
    #+end_src

    #+RESULTS[aecd9c9a3c0fb0a11cc39024efdaa645546b54ce]:
    [[file:plots/ridgeline.pdf]]

    The distributions have three distinct clusters and are very similar. 92
    shifts towards higher ~totalTime~ while 96 looks very similar to 810.
    Differences between branches are too difficult to observe with this density
    smoothing. Let's take another look with a Q-Q plot:

    TODO

*** Are the branches significant

First let's check that there is a difference between GHC versions:

#+begin_src R :exports both :results output
kruskal.test(totalTime ~ GHC, data = df)
#+end_src

Now to check if the branches have had a statistically meaningful impact while
controlling for the GHC version:

- 96

    #+begin_src R :exports both :results output
    kruskal.test(totalTime ~ Branch, data = df %>% filter(GHC == 96))
    #+end_src

- 92

    #+begin_src R :exports both :results output
    kruskal.test(totalTime ~ Branch, data = df %>% filter(GHC == 92))
    #+end_src

- 810

    #+begin_src R :exports both :results output
    kruskal.test(totalTime ~ Branch, data = df %>% filter(GHC == 810))
    #+end_src

For each version of GHC, we find p-values of less than 0.05 meaning that the
branches have had a statistically significant impact on ~totalTime~.

*** How are the branches significant

Now we'll use a pairwise wilcox to check

#+begin_src R :exports both :results output
pairwise.wilcox.test(df$totalTime, filter(df,GHC == 96)$Branch, p.adjust.method = "holm", paired = TRUE)
#+end_src

#+RESULTS[44078b0bfa6f3488d09e0a2f4d108a54da3a1dfd]:
#+begin_example

	Pairwise comparisons using Wilcoxon signed rank test with continuity correction

data:  df$totalTime and filter(df, GHC == 96)$Branch

            baseline             SplitUMElem
SplitUMElem 0.000000023          -
NoFailT     < 0.0000000000000002 < 0.0000000000000002

P value adjustment method: holm
#+end_example

Focusing on the first column which compares the branches ~SplitUMElem~ and
~NoFailT~ to the baseline, we find that both have a p-value less than 0.05 which
is what we expect. Now we'll change the test to see which branch is greater than
the baseline:

#+begin_src R :exports both :results output
pairwise.wilcox.test(df$totalTime, filter(df,GHC == 96)$Branch, p.adjust.method = "holm", paired = TRUE, alternative = "greater")
#+end_src

#+RESULTS[36b553313184695c5a0fe0f11e40d9f278a3c3fd]:
#+begin_example

	Pairwise comparisons using Wilcoxon signed rank test with continuity correction

data:  df$totalTime and filter(df, GHC == 96)$Branch

            baseline    SplitUMElem
SplitUMElem 0.000000034 -
NoFailT     1           1

P value adjustment method: holm
#+end_example

Here I have set the alternative hypothesis to be "branches are greater than the
baseline", so a p-value above 0.05 means we'll reject the alternative hypothesis
and accept the null: "that the branches are not greater than the baseline".

We see that ~SplitUMElem~ has a p-value less than
