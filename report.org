# -*- org-latex-minted-options: (("breaklines" "true") ("breakanywhere" "true") ("fontsize" "\\footnotesize")); -*-
#+title: Cardano 92 Performance Regression Follow-up
#+author: IOG DevX
#+latex_class_options: [10pt]
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage{xcolor}
#+latex_header_extra: \definecolor{LightGray}{gray}{.96}
#+latex_header_extra: \setminted{bgcolor=LightGray}
#+PROPERTY: header-args:R :session *cardano-perf-report* :cache yes :dir ./
#+MACRO: g810 GHC-8.10.7
#+macro: g92  GHC-9.2
#+macro: g96  GHC-9.6.2

* Executive Summary

- From inspecting the 92 regression the DevX team made several recommendations
  for code changes to ~cardano-ledger-core~.

- This document tests two of these changes with ~beacon~ and ~db-analyzer~ on
  {{{g810}}}, {{{g92}}}, and {{{g96}}}, and is only concerned with total
  execution time (wall time).

- Our findings are:

  - ~beacon~ and ~db-analyzer~ do observe changes in ~cardano-core-ledger~.

  - This method does _independently_ observe the {{{g92}}} regression.

  - Removing ~FailT~ has a negative impact on performance. It is not worth
    further testing.

  - Splitting ~UMElem~ to take better advantage of pointer tagging improves
    performance by 3% on {{{g96}}}. Compared to the {{{g810}}} baseline, the
    {{{g96}}} ~SplitUMElem~ branch improves performance by ~5% for 70% of the
    data; for the very slowest slots it slightly regresses the baseline on
    {{{g96}}}.

  - For the slowest slots in the dataset, {{{g96}}} outperforms {{{g810}}} with a
    13% improvement.


* Background

** General Goal

- The larger goal is to be able to predict the performance of the ledger
  operations before shipping the code.

- ~db-analyzer~  and ~beacon~ are attempts to do this by replaying the
  state changes that took place. They replay the ledger state by using ledger
  operations to make queries on the ledger state.

** ~db-analyzer~ and ~beacon~

- db-analyzer is the tool that is doing the work

- beacon is just a convenient wrapper for data generation and plotting.

* Methodology

- This data is generated by [[https://github.com/input-output-hk/ouroboros-consensus/tree/main/ouroboros-consensus-cardano#saving-a-snapshot][db-analyzer]] and [[https://github.com/input-output-hk/ouroboros-consensus-tools][beacon]]. Using a handcrafted chainDB
  from the P&T team. The handcrafted chainDB is constructed to be more dense (a
  ~k~ value of 3) than ~mainnet~ and thus should be a stress test of the ledger ops.

- The handcrafted chainDB has two blocks, which yields 

    #+name: num-observations
    #+begin_src R
    nrow(df810_baseline)
    #+end_src

    #+RESULTS[0b5c315f8449b719e466662d9e074aa6b4aee56b]: num-observations
    : 122

    call_num-observations() observations per run.

  - Furthermore, each observation belongs to a specific slot id, which means we
    have statistically /paired/ data. Paired data occurs when an observation is
    not independent between runs of an experiment. For example, if one was
    testing a diet pill on 10 mice, the dataset will have 10 observations before
    the diet pill, and 10 after, for a total of 20 observations. Each
    observation belongs to a unique mouse and the experiment is interested in
    how the pill changed each mouse individually, e.g., how did mouse 1 change.
    Thus comparing mouse 1 before the pill to mouse 4 after the pill is not a
    meaningful comparison. Similarly, comparing mouse 1 before the pill to mouse
    3 before the pill is not meaningful.

    We are dealing with a similar kind of data. Each observation belongs to a
    unique and not-independently sampled slot id. For example, slot id 63 might
    always be more performance intensive than slot 73, and so it would be
    incorrect to use statistical tests that assume a random sample. Thus we must
    use statistical tests that account for paired data.

  - Next this data is not normally distributed:

    #+name: normality-test
    #+begin_src R :exports both :results output
    shapiro.test(df810_baseline$totalTime)
    #+end_src

    #+RESULTS[c42759cb933e6bc6f606d1f2d7b31213628a564f]: normality-test
    : 
    : 	Shapiro-Wilk normality test
    : 
    : data:  df810_baseline$totalTime
    : W = 0.42007, p-value < 0.00000000000000022

    The baseline data for ~totalTime~ fails the normality test with a p-value of
    $2.2\mathrm{e}{-15}$ (this is the relevant bit: ~p-value <
    0.00000000000000022~) Even if we sample the data to exploit the central limit
    theorem we get a non-normal sample:

    #+name: normality-test-sample
    #+begin_src R :exports both :results output
    ## Here I take 50 samples from the data set
    shapiro.test(sample_n(df810_baseline, 50)$totalTime)
    #+end_src

    #+RESULTS[ecd0c92affca7c988ce9a3c90a8c0444b2b66187]: normality-test-sample
    : 
    : 	Shapiro-Wilk normality test
    : 
    : data:  sample_n(df810_baseline, 50)$totalTime
    : W = 0.50456, p-value = 0.000000000009435

    Thus we must use a [[https://www.statology.org/kruskal-wallis-test/][kruskall-wallace]] test to test if a change has a
    statistically significant effect and a [[http://sthda.com/english/wiki/paired-samples-wilcoxon-test-in-r][pairwise-wilcox-test]] to determine
    which change had which effect and in what direction (slower or faster).
    These are the /non-parametric/ versions of the regular [[https://en.wikipedia.org/wiki/Student's_t-test][t-test]] and [[https://en.wikipedia.org/wiki/Analysis_of_variance][ANOVA]]
    analyses.


* Introduction

    We'll be comparing three ghc versions: {{{g810}}}, {{{g92}}}, and {{{g96}}};
    across three branches: the baseline, split UMElem, and removing the FailT
    library.

** The baseline

    The baseline branch is set to ouroboros-consensus commit
    ~e3917f684e8b60e7bfc453d6d8114b800bdf167d~, which is the release for
    ~node-8.5~. 

** Split UMElem

    The ledger uses a map data structure called ~UMap~ whose range is
    represented by a type called ~UMElem~ which looks like this: 
    #+begin_src haskell :noeval
    -- So,
    -- TEEEE means none of the components are present,
    -- TFEEE means only the reward-deposit pair is present,
    -- TEFEE means only the set of pointers is present,
    -- TEEFE means only the stake pool id is present. etc.
    -- TEEEF means only the voting delegatee id is present, and
    --
    -- The pattern 'UMElem' will correctly use the optimal constructor.
    data UMElem c
      = TEEEE
      | TEEEF !(DRep c)
      | TEEFE !(KeyHash 'StakePool c)
      | TEEFF !(KeyHash 'StakePool c) !(DRep c)
      | TEFEE !(Set Ptr)
      | TEFEF !(Set Ptr) !(DRep c)
      | TEFFE !(Set Ptr) !(KeyHash 'StakePool c)
      | TEFFF !(Set Ptr) !(KeyHash 'StakePool c) !(DRep c)
      | TFEEE {-# UNPACK #-} !RDPair
      | TFEEF {-# UNPACK #-} !RDPair !(DRep c)
      | TFEFE {-# UNPACK #-} !RDPair !(KeyHash 'StakePool c)
      | TFEFF {-# UNPACK #-} !RDPair !(KeyHash 'StakePool c) !(DRep c)
      | TFFEE {-# UNPACK #-} !RDPair !(Set Ptr)
      | TFFEF {-# UNPACK #-} !RDPair !(Set Ptr) !(DRep c)
      | TFFFE {-# UNPACK #-} !RDPair !(Set Ptr) !(KeyHash 'StakePool c)
      | TFFFF {-# UNPACK #-} !RDPair !(Set Ptr) !(KeyHash 'StakePool c) !(DRep c)
      deriving (Eq, Ord, Generic, NoThunks, NFData)
    #+end_src

    Notice that this data type has 16 constructors. The idea behind this branch
    is to split this data type into two types each with 8 constructors. GHC uses
    three bits to tag pointers with ~000~ reserved to check for Thunks. Thus for a
    given data type GHC will check the pointer tag for the first 7 constructors;
    which is the case for the original version. By splitting ~UMElem~ into two
    data types GHC which check the pointer tag for the first 14 constructors,
    while constructor 15 and 16 will be scrutinized by looking up the
    constructor information in the heap objects' info-table. This should be much
    faster than the 16 constructor version, even at the cost of two extra
    machine words (one word each to differentiate between the two new data
    types). You can find the patch [[https://github.com/input-output-hk/cardano-ledger/compare/master...doyougnu:cardano-ledger:wip/perf-split-umelem][here]].

** Removing FailT

    The idea behind this patch is remove the polymorphism in
    ~Cardano.Ledger.Address~. This comes straight from the DevX analysis on the
    {{{g92}}} regression which found that a major difference on {{{g92}}} was a
    lack of specialization. ~FailT~ frequently showed up in that analysis and so
    removing it should pay off /if/ the specialization was a contributing factor
    to the regression. This is especially the case because the code in
    ~Cardano.Ledger.Address~ uses a ~NOINLINE~ pragma for its ~fail~ function, which
    is known to [[https://gitlab.haskell.org/ghc/ghc/-/issues/22629][prevent specialization]]. You can find the patch [[https://github.com/input-output-hk/cardano-ledger/compare/master...doyougnu:cardano-ledger:cardano-perf-regression/no-failT][here]].

* Analysis

   This analysis was done in R version:
   #+begin_src R :exports both :results output
   R.version.string
   #+end_src

   #+RESULTS[74f5cb2c597ef179de062c6aa2ef1f5bf2f8c778]:
   : [1] "R version 4.3.1 (2023-06-16)"

   and is written in a literate programming style with inline R. All data was
   collected on a machine running:

   #+name: system
   #+begin_src sh :exports both :results output
   neofetch --stdout --color_blocks off
   #+end_src

   #+RESULTS:
   #+begin_example
   doyougnu@7thChamber
   -------------------
   OS: NixOS 23.05.20231105.aeefe20 (Stoat) x86_64
   Host: ASUSTeK COMPUTER INC. PRIME X470-PRO
   Kernel: 6.5.9-xanmod1
   Uptime: 14 days, 23 hours, 8 mins
   Packages: 928 (nix-system), 2241 (nix-user), 8 (nix-default)
   Shell: fish 3.6.1
   Resolution: 1920x1080, 1080x1920
   WM: xmonad
   Theme: Breeze-Dark [GTK2/3]
   Icons: breeze [GTK2/3]
   Terminal: .emacs-29.1-wra
   CPU: AMD Ryzen 7 2700X (16) @ 3.700GHz
   GPU: NVIDIA GeForce GTX 1080 Ti
   Memory: 7850MiB / 64218MiB

   #+end_example

   call_system()


** Loading and preparing the data

Feel free to skip this section if you are not interested in the R code.

#+begin_src R :results silent
library("ggridges")
library("tidyverse")
library("rstatix")
library("tables")

options(scipen = 999)

data_dir <- "./data/"

load_data <- function(filename, ghc, branch) {
  read_tsv(paste(data_dir, filename, sep = "")) %>%
    mutate(GHC = as.factor(ghc), Branch = as.factor(branch))
}

## time units are nanoseconds
df810_baseline <- load_data("ledger-ops-cost-e3917f684e8b60e7bfc453d6d8114b800bdf167d-haskell810-from-63-nr-blocks-100000.csv", 810, "baseline")
df92_baseline  <- load_data("ledger-ops-cost-e3917f684e8b60e7bfc453d6d8114b800bdf167d-haskell-from-63-nr-blocks-100000.csv", 92, "baseline")
df96_baseline  <- load_data("ledger-ops-cost-e3917f684e8b60e7bfc453d6d8114b800bdf167d-haskell96-from-63-nr-blocks-100000.csv", 96, "baseline")

df810Split_umelem <- load_data("ledger-ops-cost-a929cd7616668b61bea38486b1641d5d45f13442-haskell810-from-63-nr-blocks-100000.csv", 810, "SplitUMElem")
df92Split_umelem  <- load_data("ledger-ops-cost-a929cd7616668b61bea38486b1641d5d45f13442-haskell-from-63-nr-blocks-100000.csv", 92, "SplitUMElem")
df96Split_umelem  <- load_data("ledger-ops-cost-a929cd7616668b61bea38486b1641d5d45f13442-haskell96-from-63-nr-blocks-100000.csv", 96, "SplitUMElem")

df810_noFailT <- load_data("ledger-ops-cost-6dc508fd5c0ddb73e4a5e01877dfcd698b1c1bd0-haskell810-from-63-nr-blocks-100000.csv", 810, "NoFailT")
df92_noFailT  <- load_data("ledger-ops-cost-6dc508fd5c0ddb73e4a5e01877dfcd698b1c1bd0-haskell-from-63-nr-blocks-100000.csv", 92, "NoFailT")
df96_noFailT  <- load_data("ledger-ops-cost-6dc508fd5c0ddb73e4a5e01877dfcd698b1c1bd0-haskell96-from-63-nr-blocks-100000.csv", 96, "NoFailT")

df <- bind_rows(
  df810_baseline, df92_baseline, df96_baseline,
  df810Split_umelem, df92Split_umelem, df96Split_umelem,
  df810_noFailT, df92_noFailT, df96_noFailT
) %>%
  mutate(TestCase = paste(GHC, Branch, sep = "_")) %>%
  arrange(slot)
#+end_src

#+RESULTS:

** A first look at the data

Now we have our dataset, let's plot the distribution of ~totalTime~ for each
ghc and branch. I'll use a [[https://en.wikipedia.org/wiki/Ridgeline_plot][ridgeline plot]] to observe changes in the
distributions. Note that the x-axis is ~log10~ because we have an exponential
distribution:

#+begin_src R :exports both :results output graphics file :file plots/ridgeline.pdf
p <- ggplot(df, aes(totalTime,
                    y = TestCase,
                    fill = GHC)) +
    geom_density_ridges(alpha = .6) +
    scale_x_log10() +
    xlab("TotalTime [ns]") +
    ylab("GHC_Branch") +
    theme_bw()
p
#+end_src

#+RESULTS[e167b9d6fa2d3b234837eb8e8bf3f1b2b993bebf]:
[[file:plots/ridgeline.pdf]]

Each plot is a kernel density plot which shows the shape and relative position
of the distribution of ~totalTime~ for each GHC and each branch. With this plot we
are simply trying to visualize the distribution of the ~totalTime~ date. We see
that the distributions all have three distinct clusters and are similar; the
branches and GHC versions have not fundamentally changed the distribution of
~totalTime~ . {{{g92}}} shifts towards higher ~totalTime~ while {{{g96}}} looks
similar to {{{g810}}}. Differences between branches are too close to observe
with the default density smoothing (the default smoothing is for univariate data
which is the kind of data we are dealing with).


** Are the versions significant

First let's check that there is a difference between GHC versions:

#+begin_src R :exports both :results output
kruskal.test(totalTime ~ GHC, data = df)
#+end_src

#+RESULTS[198ed04a9ec3b12efeef1696bd50ef00da0c82e9]:
:
: 	Kruskal-Wallis rank sum test
:
: data:  totalTime by GHC
: Kruskal-Wallis chi-squared = 70.109, df = 2, p-value =
: 0.0000000000000005969

We find a p-value of $5.9\mathrm{e}{-15}$ meaning that GHC version has a
statistically meaningful impact on ~totalTime~. Now to check if the branches have
had a statistically meaningful impact while controlling for the GHC version:

- {{{g96}}}

    #+begin_src R :exports both :results output
    kruskal.test(totalTime ~ Branch, data = df %>% filter(GHC == 96))
    #+end_src

    #+RESULTS[8d5d230faa7396301b4ce3d9ca9638b47ad49764]:
    :
    : 	Kruskal-Wallis rank sum test
    :
    : data:  totalTime by Branch
    : Kruskal-Wallis chi-squared = 12.293, df = 2, p-value = 0.00214

- {{{g92}}}

    #+begin_src R :exports both :results output
    kruskal.test(totalTime ~ Branch, data = df %>% filter(GHC == 92))
    #+end_src

    #+RESULTS[c3e5939a912e77ec10acd9818b40235f622b6396]:
    :
    : 	Kruskal-Wallis rank sum test
    :
    : data:  totalTime by Branch
    : Kruskal-Wallis chi-squared = 14.716, df = 2, p-value = 0.0006376

- {{{g810}}}

    #+begin_src R :exports both :results output
    kruskal.test(totalTime ~ Branch, data = df %>% filter(GHC == 810))
    #+end_src

    #+RESULTS[2cce8d84c1f4fcc1a4768f3794d95fbda4276dc2]:
    :
    : 	Kruskal-Wallis rank sum test
    :
    : data:  totalTime by Branch
    : Kruskal-Wallis chi-squared = 7.9877, df = 2, p-value = 0.01843

For each version of GHC, we find p-values of less than 0.05 meaning that the
branches have had a statistically significant impact on ~totalTime~.

** How are the branches significant

Now we'll use a pairwise wilcox to check which branches differ from the
baseline. We'll just test with {{{g96}}} for now and return to the other GHC
versions:

#+begin_src R :exports both :results output
pairwise.wilcox.test(df$totalTime, filter(df,GHC == 96)$Branch, p.adjust.method = "holm", paired = TRUE)
#+end_src

#+RESULTS[44078b0bfa6f3488d09e0a2f4d108a54da3a1dfd]:
#+begin_example

	Pairwise comparisons using Wilcoxon signed rank test with continuity correction

data:  df$totalTime and filter(df, GHC == 96)$Branch

            baseline             SplitUMElem
SplitUMElem 0.000000023          -
NoFailT     < 0.0000000000000002 < 0.0000000000000002

P value adjustment method: holm
#+end_example

The first column compares the branches ~SplitUMElem~ and ~NoFailT~ to the ~baseline~,
we find that both have a p-value less than 0.05 meaning that both branches are
statistically different from the baseline for {{{g96}}}. Now we'll compare the
branches for each ghc version explicitly:

#+begin_src R :exports both :results output
pairwise.wilcox.test(df$totalTime, filter(df,GHC == 92)$Branch, p.adjust.method = "holm", paired = TRUE)
#+end_src

      #+RESULTS[d1e166e82682da939659a7937c09a3b909df5a66]:
      #+begin_example

          Pairwise comparisons using Wilcoxon signed rank test with continuity correction

      data:  df$totalTime and filter(df, GHC == 92)$Branch

                  baseline             SplitUMElem
      SplitUMElem 0.000000023          -
      NoFailT     < 0.0000000000000002 < 0.0000000000000002

      P value adjustment method: holm
      #+end_example

#+begin_src R :exports both :results output
pairwise.wilcox.test(df$totalTime, filter(df,GHC == 810)$Branch, p.adjust.method = "holm", paired = TRUE)
#+end_src

#+RESULTS[d4cb93bba32d40e622a12adb6e9945debac0c2d6]:
#+begin_example

Pairwise comparisons using Wilcoxon signed rank test with continuity correction

data:  df$totalTime and filter(df, GHC == 810)$Branch

        baseline             SplitUMElem
SplitUMElem 0.000000023          -
NoFailT     < 0.0000000000000002 < 0.0000000000000002

P value adjustment method: holm
#+end_example

And we can see that both branches are meaningfully different from the baseline
for all versions of GHC.

Now we'll see /how/ they differ, we'll calculate the median ~totalTime~ and
[[https://en.wikipedia.org/wiki/Interquartile_range][interquartile range]] by GHC version and branch to observe how each branch has
impacted ~totalTime~ (note that we use the median because we have an exponential
distribution, thus the mean would be heavily skewed by the extreme outliers in
the dataset):

#+begin_src R :exports both :results output
df %>%
group_by(GHC,Branch) %>%
select(totalTime) %>%
get_summary_stats(type = "median_iqr")
#+end_src

#+RESULTS[840511434a46be387404e2b75018f96f18389c03]:
#+begin_example
Adding missing grouping variables: `GHC`, `Branch`
# A tibble: 9 × 6
GHC   Branch      variable      n median    iqr
<fct> <fct>       <fct>     <dbl>  <dbl>  <dbl>
1 810   baseline    totalTime   122 32200. 37113
2 810   SplitUMElem totalTime   122 33083  31973.
3 810   NoFailT     totalTime   122 32521  71903.
4 92    baseline    totalTime   122 65250. 39085.
5 92    SplitUMElem totalTime   122 64412. 38234.
6 92    NoFailT     totalTime   122 68834. 41404.
7 96    baseline    totalTime   122 32088. 28964.
8 96    SplitUMElem totalTime   122 30942. 27022.
9 96    NoFailT     totalTime   122 32738  28118.
#+end_example

Let's begin with {{{g96}}}; the last three rows. We can see that ~SplitUMElem~
median execution time is 30942 nanoseconds, compared to the baseline median of
32088, a difference of 1146 nanoseconds or 1 millisecond (an improvement of 3%).
Similarly we can see that the inter-quartile range of ~SplitUMElem~ has reduced by
1942 nanoseconds or (2 ms). This means that the ~SplitUMElem~ distribution is
tighter than the baseline and consequently the performance has become more
[[https://en.wikipedia.org/wiki/Accuracy_and_precision][precise]]. Let's check the distributions outside of the interquartile range to
observe the best and worst performing slots:

#+begin_src R :exports both :results output
df %>%
group_by(GHC,Branch) %>%
reframe(enframe(quantile(totalTime, c(0.05,0.1,0.5,0.9,0.95)), "quantile", "totalTime")) %>%
pivot_wider(names_from = quantile, values_from = totalTime)
#+end_src

#+RESULTS[62a55f0e04118e0f423c43996543df6872c42fe5]:
#+begin_example
# A tibble: 9 × 7
  GHC   Branch        `5%`  `10%`  `50%`   `90%`    `95%`
  <fct> <fct>        <dbl>  <dbl>  <dbl>   <dbl>    <dbl>
1 810   baseline    12490. 12521. 32200. 516207  1288644.
2 810   SplitUMElem 12901. 12982. 33083  564792. 1485672.
3 810   NoFailT     12181. 12260. 32521  553308. 1617108.
4 92    baseline    24130. 24190. 65250. 532394. 1323829.
5 92    SplitUMElem 23914. 24007. 64412. 460273. 1149916.
6 92    NoFailT     26324. 26399. 68834. 497695. 1238604.
7 96    baseline    12225. 12261. 32088. 407903. 1122081.
8 96    SplitUMElem 11842. 11890. 30942. 414497  1131364.
9 96    NoFailT     12291. 12328. 32738  455974. 1440405.
#+end_example

In this table we have the 5th, 10th, 50th (median), 90th, and 95th percentile by
GHC version and branch. There are several notable things:

- {{{g96}}} ~SplitUMElem~ is consistently better than baseline /until/ the 90th
  percentile.

- ~NoFailT~ consistently grows more rapidly than baseline /except/ on {{{g92}}}. It's
  likely that the signal is obscured by something else an {{{g92}}} given that
  all data on {{{g92}}} shifts regardless of branch.

- ~baseline~ is consistently the best performing branch on {{{g810}}}.

- The median values between {{{g810}}} and {{{g96}}} are basically identical
  (except ~SplitUMElem~), but the top end of the distribution (i.e. the slowest
  slots): 90th percentile and above show a drastic improvement with {{{g96}}}
  compared to {{{g810}}}. For example, the 95th percentile for ~baseline~ on
  {{{g96}}} is 1122081 compared to 1288644, an improvement of 13%.

The speedup at the upper tail of the distribution is interesting. Let's
calculate the speedup of the distribution for each GHC version and branch and
plot them:

#+begin_src R :exports both :results output :cache no
speedup_df <- df %>%
    group_by(GHC,Branch) %>%
    reframe(enframe(quantile(totalTime, seq(0,1,0.1)), "quantile", "totalTime")) %>%
    pivot_wider(names_from = GHC, values_from = totalTime, names_prefix = "GHC") %>%
    mutate(speedup96 = ((GHC810 - GHC96) / GHC810) * 100
        ,speedup92 = ((GHC810 - GHC92) / GHC810) * 100
        ,percentile = as.numeric(substr(quantile,1, nchar(quantile)-1)))

speedup_df
#+end_src

#+RESULTS[f2a29d9de410e41f8a395a5573e9ac85b5baa597]:
#+begin_example
# A tibble: 33 × 8
   Branch   quantile  GHC810   GHC92   GHC96 speedup96 speedup92 percentile
   <fct>    <chr>      <dbl>   <dbl>   <dbl>     <dbl>     <dbl>      <dbl>
 1 baseline 0%        12359   23862   12078      2.27     -93.1           0
 2 baseline 10%       12521.  24190.  12261.     2.08     -93.2          10
 3 baseline 20%       12602.  24615.  12354.     1.97     -95.3          20
 4 baseline 30%       12832.  27776   12600.     1.80    -116.           30
 5 baseline 40%       31815.  46769.  27762.    12.7      -47.0          40
 6 baseline 50%       32200.  65250.  32088.     0.348   -103.           50
 7 baseline 60%       33190.  65774   32360.     2.50     -98.2          60
 8 baseline 70%       35470.  66135.  33221      6.34     -86.5          70
 9 baseline 80%      482283. 494415. 379200.    21.4       -2.52         80
10 baseline 90%      516207  532394. 407903.    21.0       -3.14         90
# ℹ 23 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_example

and now to plot, we'll only focus on {{{g96}}} because {{{g92}}} clearly
regresses:

#+begin_src R :exports both :results output graphics file :file speedup_quantiles.pdf :cache no
p <- ggplot(speedup_df %>%
              select(!speedup92) %>%
              pivot_longer(cols = starts_with("speedup"),names_to = "comparison", values_to = "speedup")
           , aes(x = percentile, y = speedup, color = Branch, shape = Branch)) +
  geom_point(size = 3) +
  scale_shape_manual(values=c(16,15,17)) +
  scale_color_manual(values=c("red", "blue", "green")) +
  scale_y_continuous(breaks = seq(0,35,5)) +
  scale_x_continuous(breaks = seq(0,100,10)) +
  ylab("Speedup %") +
  xlab("Percentile of totalTime") +
  ggtitle("Speedup of GHC96 over GHC810 by Branch") +
  theme_bw()

p
#+end_src

#+RESULTS[4b5fdb43b6afca9d77149348fc5825f5f841dbb0]:
[[file:speedup_quantiles.pdf]]

This plot shows the speedup of {{{g96}}} compared to {{{g810}}} for all branches
at each 10th percentile of the ~totalTime~ distribution. For example, at the
median (50th percentile) we see ~baseline~ with a speedup of 0% while ~SplitUMElem~
shows a speedup of 7% at the median. This means that at the median of the
~totalTime~ distribution the ~baseline~ did not improve /on {{{g96}}}/ while
~SplitUMElem~ did by 7%. Note that a negative value indicates a slowdown. We see
that each branch, even ~baseline~ experience a speedup of {{{g96}}} over {{{g810}}}.

The takeaway from this plot is that the upper tail of the distribution, that is,
the slowest slots in the dataset, experience the largest improvement on {{{g96}}}
over {{{g810}}}. Furthermore ~SplitUMElem~ is particularly sensitive showing an
improvement of 27% at the 80th percentile and 7-10% improvement for the rest of
the distribution (compared to 0-2% improvement for the ~baseline~). This implies
that {{{g96}}} better optimizes the ~SplitUMElem~ branch.

To wrap up, we'll create the same speedup plot but instead of showing the
speedup of each branch on {{{g96}}} compared to {{{g810}}}, we'll compare each
branch on {{{g96}}} to only the baseline of {{{g810}}}:

#+begin_src R :exports both :results output :cache no
calc_speedup <- function(baseline,branch) {
  ((baseline - branch) / baseline) * 100
}

speedup_baseline <- df %>%
  group_by(GHC,Branch) %>%
  reframe(enframe(quantile(totalTime, seq(0,1,0.1)), "quantile", "totalTime")) %>%
  pivot_wider(names_from = c(Branch,GHC), values_from = totalTime, names_sep = "") %>%
  mutate(baseline_92    = calc_speedup(baseline810,baseline92)
       , baseline_96    = calc_speedup(baseline810,baseline96)
       , splitUMElem_92 = calc_speedup(baseline810,SplitUMElem92)
       , splitUMElem_96 = calc_speedup(baseline810,SplitUMElem96)
       , noFailT_92     = calc_speedup(baseline810,NoFailT92)
       , noFailT_96     = calc_speedup(baseline810,NoFailT96)
       , percentile = as.numeric(substr(quantile,1, nchar(quantile)-1))
       ) %>%
  select(percentile,contains("_")) %>%
  pivot_longer(cols = contains("_"), names_to = c("Branch", "GHC"), values_to = "speedup", names_sep="_") %>%
  mutate(Branch = as.factor(Branch)
         , GHC  = as.factor(GHC))

speedup_baseline
#+end_src

#+RESULTS[b8ef4bb3b716b78315e4308814c7956eae2134e4]:
#+begin_example
# A tibble: 66 × 4
   percentile Branch      GHC   speedup
        <dbl> <fct>       <fct>   <dbl>
 1          0 baseline    92     -93.1
 2          0 baseline    96       2.27
 3          0 splitUMElem 92     -92.8
 4          0 splitUMElem 96       4.76
 5          0 noFailT     92    -110.
 6          0 noFailT     96       1.11
 7         10 baseline    92     -93.2
 8         10 baseline    96       2.08
 9         10 splitUMElem 92     -91.7
10         10 splitUMElem 96       5.04
# ℹ 56 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_example


and now the plot:

#+begin_src R :exports both :results output graphics file :file speedup_v_810_quantiles.pdf :cache no
p <- speedup_baseline %>%
              pivot_longer(cols = contains("Speedup"),names_to = "comparison", values_to = "speedup") %>%
              arrange(desc(Branch)) %>%
  ggplot(aes(x = percentile, y = speedup, color = Branch, shape = Branch)) +
  geom_point(size = 3) +
  facet_grid(GHC ~ ., scales = "free_y") +
  scale_shape_manual(values=c(16,17,15)) +
  scale_color_manual(values=c("red", "green", "blue")) +
  scale_x_continuous(breaks = seq(0,100,10)) +
  ylab("Speedup %") +
  xlab("Percentile of totalTime") +
  ggtitle("Speedup by GHC version and Branch with respect to GHC-810 Baseline") +
  theme_bw()

p
#+end_src

#+RESULTS[ce9658bd735bf1916f36ea776332b04cf78aafcd]:
[[file:speedup_v_810_quantiles.pdf]]

This is a faceted plot, the top subplot shows the speedup relative to the
baseline of {{{g810}}} for {{{g92}}}, notice that the y-axis is negative, i.e.,
{{{g92}}} regresses. The bottom subplot shows the same speedup for {{{g96}}}. We
see that ~SplitUMElem~ consistently shows more speedup over the baseline of
{{{g810}}} except at the 40th percentile and above the 80th percentile where it
matches the {{{g96}}} baseline. Note the subtle difference in this plot versus
the last plot. In this plot we compare ~SplitUMElem~ on {{{g96}}} to the
{{{g810}}} ~baseline~, whereas in the last plot we compared ~SplitUMElem~ on
{{{g96}}} against ~SplitUMElem~ on {{{g810}}}. Thus we have two conclusions:
first ~SplitUMElem~ experiences a larger speedup from {{{g96}}} than other
branches; and second, that ~SplitUMElem~ performs better than both the {{{g96}}}
and {{{g810}}} baseline until top 20 percent of the ~totalTime~ distribution.

Therefore, whether to use ~SplitUMElem~ or not is a tradeoff: gain a 5%
performance bump for the majority of slots in the sample at the cost of a slight
regression for the absolutely slowest slots.
